{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7a8129c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6f/k2t20zbs4db5khp6mgdq_06m0000gn/T/ipykernel_25917/2187368035.py:3: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from rdflib import Graph, URIRef, Namespace, Literal, XSD, RDF\n",
    "from rdflib.plugins.stores.sparqlstore import SPARQLStore\n",
    "from tqdm.autonotebook import tqdm\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "import numpy\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "from rapidfuzz import process, fuzz, distance\n",
    "import math\n",
    "\n",
    "import graph_similarity\n",
    "\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import concurrent.futures\n",
    "\n",
    "from rdflib import URIRef, Literal\n",
    "import string\n",
    "import dateutil.parser as dparser\n",
    "import datetime\n",
    "from rapidfuzz import fuzz, distance\n",
    "import math\n",
    "import re\n",
    "import os\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "14f954bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAPH_1 = \"/Users/yamamotobikutorueiichi/codes/food_base/agrovoc_processed.ttl\"\n",
    "GRAPH_1_INV_FUNC_PATH = '/Users/yamamotobikutorueiichi/codes/food_base/agrovoc_inversability.csv'\n",
    "\n",
    "#GRAPH_2 = \"/Users/yamamotobikutorueiichi/codes/custom_paris/mbt.xml\"\n",
    "GRAPH_2_INV_FUNC_PATH = '/Users/yamamotobikutorueiichi/codes/gollumn - subdataset/dbpedia-inversability.csv'\n",
    "\n",
    "GRAPH_2 = \"http://localhost:8890/sparql\"\n",
    "SUBJECT = 'subject'\n",
    "PREDICATE = 'predicate'\n",
    "INVERSE_FUNCTIONALITY = 'inverse_functionality'\n",
    "INVERSABILITY = 'inverse_functionality'\n",
    "FUNCTIONALITY = 'functionality'\n",
    "MAX_LENGTH_FULL_MATCH = 10000\n",
    "MAX_SAVED_PREDICATE_COUNTER = 10000000\n",
    "LABEL_PREDICATE = \"http://www.w3.org/2000/01/rdf-schema#label\"\n",
    "FILE_FOLDER = \"/Users/yamamotobikutorueiichi/codes/custom_paris/both func and func inv method/early_stop/agrovoc-dbpedia/\"\n",
    "GRAPH_1_ABSTRACT = \"http://dbkwik.webdatacommons.org/ontology/abstract\"\n",
    "GRAPH_2_ABSTRACT = \"http://dbkwik.webdatacommons.org/ontology/abstract\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e08032c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Graph identifier=Ne8f7e102700f4bba99858692005c7627 (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_1_inv_func_df = pd.read_csv(GRAPH_1_INV_FUNC_PATH)\n",
    "\n",
    "#graph_1_predicate_list = threshold_graph_1_inv_func_df[PREDICATE].tolist()\n",
    "\n",
    "graph_1 = Graph()\n",
    "graph_1.parse(GRAPH_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a00a7c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_2_inv_func_df = pd.read_csv(GRAPH_2_INV_FUNC_PATH)\n",
    "\n",
    "#graph_2_predicate_list = threshold_graph_2_inv_func_df[PREDICATE].tolist()\n",
    "\n",
    "#graph_2 = Graph()\n",
    "#graph_2.parse(GRAPH_2)\n",
    "graph_2 = Graph(\"SPARQLStore\")\n",
    "graph_2.open(GRAPH_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "af0da585",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_part_url(uri):\n",
    "    return uri.rsplit('/', 1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5a9260ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_graph(graph, predicate_df, uri_column):\n",
    "    predicate_label_list = list()\n",
    "    LABEL = \"http://www.w3.org/2000/01/rdf-schema#label\"\n",
    "    ALT_LABEL = \"http://www.w3.org/2004/02/skos/core#altLabel\"\n",
    "    label_predicate = URIRef(LABEL)\n",
    "    alt_label_predicate = URIRef(ALT_LABEL)\n",
    "    for index, row in tqdm(predicate_df.iterrows(), total=len(predicate_df)):\n",
    "        search_uriref = URIRef(row[uri_column])\n",
    "        label_list = list()\n",
    "        for label in graph.objects(search_uriref, label_predicate):\n",
    "            label_list.append(str(label))\n",
    "        for label in graph.objects(search_uriref, alt_label_predicate):\n",
    "            label_list.append(str(label))\n",
    "        if len(label_list) == 0:\n",
    "            label_list.append(get_last_part_url(row[uri_column]))\n",
    "        predicate_label_list.append(label_list)\n",
    "    return predicate_label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ed386ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity_using_label(df_1, df_2, uri_column):\n",
    "    label_list_1 = get_label_graph(graph_1, df_1, uri_column)\n",
    "    df_1['label'] = label_list_1\n",
    "    \n",
    "    label_list_2 = get_label_graph(graph_2, df_2, uri_column)\n",
    "    df_2['label'] = label_list_2\n",
    "    \n",
    "    uri_set_1 = set(df_1[uri_column])\n",
    "    uri_set_2 = set(df_2[uri_column])\n",
    "    \n",
    "    prob_dict = dict()\n",
    "    \n",
    "    for uri_1 in uri_set_1:\n",
    "        if uri_1 in uri_set_2:\n",
    "            prob_dict.setdefault(uri_1, {})\n",
    "            prob_dict[uri_1][uri_1] = 1.0\n",
    "    \n",
    "    return prob_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f61e172e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_labels(prob_dict, df_1, df_2, label_column, max_score, uri_column):\n",
    "    unique_labels = set()\n",
    "    for label_list in df_1[label_column]:\n",
    "        unique_labels.update(label_list)\n",
    "    #print(unique_labels)\n",
    "    df_1_label_dict = {label: [] for label in unique_labels}\n",
    "    for index, row in df_1.iterrows():\n",
    "        for label in row[label_column]:\n",
    "            df_1_label_dict[label].append(row[uri_column])\n",
    "    \n",
    "    if '' in df_1_label_dict:\n",
    "        del df_1_label_dict['']\n",
    "    \n",
    "    for _, row in tqdm(df_2.iterrows(), total=len(df_2), desc=\"match labels\"):\n",
    "        labels = row[label_column]\n",
    "        uri_2 = row[uri_column]\n",
    "        for label in labels:\n",
    "            if label in df_1_label_dict:\n",
    "                for uri_1 in df_1_label_dict[label]:\n",
    "                    if uri_1 in prob_dict and uri_2 in prob_dict[uri_1]:\n",
    "                        continue\n",
    "                    prob_dict.setdefault(uri_1, {})[uri_2] = max_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "32d2617c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_string(s):\n",
    "    # remove parenthesis\n",
    "    s = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", s)\n",
    "    # remove punctuation\n",
    "    s = s.translate(str.maketrans('', '', string.punctuation))\n",
    "    # split the string at uppercase letters and digits and join with spaces\n",
    "    s = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', s)\n",
    "    s = re.sub(r'([A-Z])([A-Z][a-z])', r'\\1 \\2', s)\n",
    "    s = re.sub(r'([a-zA-Z])(\\d)', r'\\1 \\2', s)\n",
    "    s = re.sub(r'(\\d)([a-zA-Z])', r'\\1 \\2', s)\n",
    "    # replace underscores with spaces\n",
    "    s = s.replace('_', ' ')\n",
    "    # remove multiple consecutive spaces and leading/trailing spaces\n",
    "    s = ' '.join(s.split()).strip()\n",
    "    # convert to lowercase and return\n",
    "    return s.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7892f192",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/yamamotobikutorueiichi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords') # download stop words list\n",
    "\n",
    "stop_words = set(stopwords.words('english')) # set of English stop words\n",
    "\n",
    "def remove_stopwords(s):\n",
    "    words = s.split() # split text into individual words\n",
    "\n",
    "    filtered_words = [word for word in words if not word.lower() in stop_words] # remove stop words\n",
    "\n",
    "    filtered_text = ' '.join(filtered_words)\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "54b13ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_corpus_specific_stopword(graph_df):\n",
    "    doc_freq = defaultdict(int)\n",
    "\n",
    "    for _, row in graph_df.iterrows():\n",
    "        doc = ' '.join(row['stopword label'])\n",
    "        words = set(word_tokenize(doc))\n",
    "        for word in words:\n",
    "            doc_freq[word] += 1\n",
    "            \n",
    "    total_docs = len(graph_df)\n",
    "    corpus_stopwords = set()\n",
    "    for word, freq in doc_freq.items():\n",
    "        if freq / total_docs > 0.2: # word appears in more than 20% of documents\n",
    "            corpus_stopwords.add(word)    \n",
    "    label_list = list()\n",
    "    for _, row in graph_df.iterrows():\n",
    "        filtered_text_list = list()\n",
    "        for label in row['stopword label']:\n",
    "            words = word_tokenize(label)\n",
    "            filtered_words = [word for word in words if not word.lower() in corpus_stopwords]\n",
    "            filtered_text = ' '.join(filtered_words)\n",
    "            filtered_text_list.append(filtered_text)\n",
    "        label_list.append(filtered_text_list)\n",
    "    \n",
    "    graph_df['corpus stopword label'] = label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b8b73e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/yamamotobikutorueiichi/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "\n",
    "nltk_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c52e6bf4-f5bb-4128-b495-e513004b999c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "\n",
    "#model_dir = '/Users/yamamotobikutorueiichi/codes/models/stella_en_1.5B_v5'\n",
    "#model_dir = \"dunzhang/stella_en_1.5B_v5\"\n",
    "#model = AutoModel.from_pretrained(model_dir).to(device)\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8540323a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_long_sentence(long_sentence):\n",
    "    sentences = nltk_tokenizer.tokenize(long_sentence)\n",
    "    \n",
    "    tokens = {'input_ids': [], 'attention_mask': []}\n",
    "\n",
    "    max_token = 140\n",
    "    #for sentence in sentences:\n",
    "    #    new_tokens = tokenizer.encode_plus(sentence, return_tensors=\"pt\", max_length=max_token, padding='max_length')\n",
    "    #    tokens['input_ids'].append(new_tokens['input_ids'][0][:max_token])\n",
    "    #    tokens['attention_mask'].append(new_tokens['attention_mask'][0][:max_token])\n",
    "\n",
    "    #tokens['input_ids'] = torch.stack(tokens['input_ids'])\n",
    "    #tokens['attention_mask'] = torch.stack(tokens['attention_mask'])\n",
    "\n",
    "    # Process sentences\n",
    "    for sentence in sentences:\n",
    "        new_tokens = tokenizer.encode_plus(sentence, return_tensors=\"pt\", max_length=max_token, padding='max_length')\n",
    "        tokens['input_ids'].append(new_tokens['input_ids'][0][:max_token])\n",
    "        tokens['attention_mask'].append(new_tokens['attention_mask'][0][:max_token])\n",
    "    \n",
    "    # Stack and move tokens to the MPS device\n",
    "    tokens['input_ids'] = torch.stack(tokens['input_ids']).to(device)\n",
    "    tokens['attention_mask'] = torch.stack(tokens['attention_mask']).to(device)\n",
    "\n",
    "    \n",
    "    outputs = model(**tokens)\n",
    "    \n",
    "    embeddings = outputs.last_hidden_state\n",
    "    \n",
    "    attention_mask = tokens['attention_mask']\n",
    "    \n",
    "    mask = attention_mask.unsqueeze(-1).expand(embeddings.size()).float()\n",
    "\n",
    "    masked_embeddings = embeddings * mask\n",
    "    \n",
    "    summed = torch.sum(masked_embeddings, 1)\n",
    "\n",
    "    summed_mask = torch.clamp(mask.sum(1), min=1e-9)\n",
    "\n",
    "    mean_pooled = summed / summed_mask\n",
    "    \n",
    "    return mean_pooled.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f48c0a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_vec_dict(df):\n",
    "    LABEL = 'corpus stopword label'\n",
    "\n",
    "    #df_label_set = set(df[LABEL])\n",
    "    unique_labels = set().union(*df[LABEL])\n",
    "\n",
    "    df_label_vec_dict = dict()\n",
    "\n",
    "    for label in tqdm(unique_labels):\n",
    "        if not label:\n",
    "            continue\n",
    "\n",
    "        df_label_vec_dict[label] = embed_long_sentence(label)\n",
    "    return df_label_vec_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5745e1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_match_label(prob_dict, df_1, df_2, uri_column, label_to_apply, new_label, max_score):\n",
    "    df_1[new_label] = df_1[label_to_apply].apply(lambda label_list: [normalize_string(label) for label in label_list])\n",
    "\n",
    "    df_2[new_label] = df_2[label_to_apply].apply(lambda label_list: [normalize_string(label) for label in label_list])\n",
    "    \n",
    "    match_labels(prob_dict, df_1, df_2, new_label, max_score, uri_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e6c068c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_prob_dict(df_1, df_2, uri_column):\n",
    "    prob_dict = calculate_similarity_using_label(df_1, df_2, uri_column)\n",
    "    \n",
    "    match_labels(prob_dict, df_1, df_2, 'label', 0.9, uri_column)\n",
    "\n",
    "    apply_match_label(prob_dict, df_1, df_2, uri_column, 'label', 'normalized label', 0.8)\n",
    "    apply_match_label(prob_dict, df_1, df_2, uri_column, 'normalized label', 'stopword label', 0.7)\n",
    "\n",
    "    remove_corpus_specific_stopword(df_1)\n",
    "    remove_corpus_specific_stopword(df_2)\n",
    "    \n",
    "    # Fuzzy and BERT match only for list with less than MAX_LENGTH_FULL_MATCH\n",
    "    if len(df_1) > MAX_LENGTH_FULL_MATCH or len(df_2) > MAX_LENGTH_FULL_MATCH:\n",
    "        return prob_dict\n",
    "\n",
    "    # Match using fuzzy string\n",
    "    LABEL = 'corpus stopword label'\n",
    "    for _, row in tqdm(df_1.iterrows(), total=len(df_1), desc='Corpus stopword match'):\n",
    "        labels_1 = row[LABEL]\n",
    "        uri_1 = row[uri_column]\n",
    "        if not labels_1:\n",
    "            continue\n",
    "\n",
    "        for _, row_2 in df_2.iterrows():\n",
    "            labels_2 = row_2[LABEL]\n",
    "            uri_2 = row_2[uri_column]\n",
    "            if not labels_2:\n",
    "                continue\n",
    "            if not uri_1 in prob_dict.keys() or not uri_2 in prob_dict[uri_1]:\n",
    "                for label_1 in labels_1:\n",
    "                    max_sim = 0.0\n",
    "                    for label_2 in labels_2:\n",
    "                        sim = fuzz.WRatio(label_1, label_2) / 100\n",
    "                        prob = 0.7 * sim\n",
    "                        if prob > max_sim:\n",
    "                            max_sim = prob\n",
    "                    prob_dict.setdefault(uri_1, {})[uri_2] = max_sim           \n",
    "    \n",
    "    # Match using BERT\n",
    "    graph_1_label_vec_dict = get_label_vec_dict(df_1)\n",
    "    graph_2_label_vec_dict = get_label_vec_dict(df_2)\n",
    "    \n",
    "    LABEL = 'corpus stopword label'\n",
    "    for _, row in tqdm(df_2.iterrows(), total=len(df_2)):\n",
    "        if uri_1 in prob_dict and uri_2 in prob_dict[uri_1] and prob_dict[uri_1][uri_2] > 0.7:\n",
    "            continue\n",
    "                \n",
    "        labels_2 = row[LABEL]\n",
    "        uri_2 = row[uri_column]\n",
    "        if not labels_2:\n",
    "            continue\n",
    "        \n",
    "\n",
    "        for _, row_1 in df_1.iterrows():\n",
    "            uri_1 = row_1[uri_column]\n",
    "            labels_1 = row_1[LABEL]\n",
    "\n",
    "            if not labels_1:\n",
    "                continue\n",
    "            \n",
    "            max_sim = 0.0\n",
    "            for label_1 in labels_1:\n",
    "                if not label_1:\n",
    "                    continue\n",
    "                for label_2 in labels_2:\n",
    "                    if not label_2:\n",
    "                        continue\n",
    "                    vec_1 = graph_1_label_vec_dict[label_1]\n",
    "                    vec_2 = graph_2_label_vec_dict[label_2]\n",
    "\n",
    "                    similarities = cosine_similarity(vec_1, vec_2)\n",
    "                    max_similarity = max(map(max, similarities))\n",
    "                    prob = 0.7 * max_similarity\n",
    "                    if prob > max_sim:\n",
    "                        max_sim = prob\n",
    "\n",
    "            if max_sim > prob_dict[uri_1][uri_2]:\n",
    "                prob_dict[uri_1][uri_2] = max_sim\n",
    "    \n",
    "    return prob_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dde605b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f0bf8cfd8dc4a80ad16669dfa73cd95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fname = FILE_FOLDER + 'predicate_sem_prob.csv'\n",
    "predicate_prob_dict = dict()\n",
    "\n",
    "if os.path.isfile(fname):\n",
    "    predicate_sem_prob_df = pd.read_csv(fname)\n",
    "    for _, row in tqdm(predicate_sem_prob_df.iterrows(), total=len(predicate_sem_prob_df)):\n",
    "        p1 = row['p1']\n",
    "        p2 = row['p2']\n",
    "        sim = row['sim']\n",
    "        predicate_prob_dict.setdefault(p1, dict())[p2] = sim\n",
    "else:\n",
    "    predicate_prob_dict = calculate_prob_dict(graph_1_inv_func_df, graph_2_inv_func_df, 'predicate')\n",
    "    predicate_sem_prob_list = list()\n",
    "\n",
    "    for pred_1, pred_2_dict in predicate_prob_dict.items():\n",
    "        for pred_2 in pred_2_dict:\n",
    "            predicate_sem_prob_list.append({\n",
    "                'p1': pred_1,\n",
    "                'p2': pred_2,\n",
    "                'sim': pred_2_dict[pred_2]\n",
    "            })\n",
    "    predicate_sem_prob_df = pd.DataFrame(predicate_sem_prob_list)\n",
    "    predicate_sem_prob_df.to_csv(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e95d1651",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_graph_1_inv_func_df = graph_1_inv_func_df[(graph_1_inv_func_df[FUNCTIONALITY] > 0.25) | (graph_1_inv_func_df[INVERSE_FUNCTIONALITY] > 0.25)]\n",
    "threshold_graph_2_inv_func_df = graph_2_inv_func_df[(graph_2_inv_func_df[FUNCTIONALITY] > 0.25) | (graph_2_inv_func_df[INVERSE_FUNCTIONALITY] > 0.25)]\n",
    "\n",
    "#threshold_graph_1_inv_func_df = graph_1_inv_func_df\n",
    "#threshold_graph_2_inv_func_df = graph_2_inv_func_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a52edc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graph_classes(graph):\n",
    "    query = \"\"\"\n",
    "        select * {?s a owl:Class.}\n",
    "        \"\"\"\n",
    "    class_list = list()\n",
    "    \n",
    "    for res in graph.query(query):\n",
    "        class_list.append(res[0])\n",
    "    return class_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b223dce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graph_entities(graph):\n",
    "    #query = \"\"\"\n",
    "    #    PREFIX skos: <http://www.w3.org/2004/02/skos/core#>\n",
    "    #    SELECT DISTINCT ?s\n",
    "    #    WHERE {\n",
    "    #      ?s a ?type.\n",
    "    #      FILTER NOT EXISTS {\n",
    "    #        ?type rdf:type skos:Concept.\n",
    "    #      }\n",
    "    #    }\"\"\"\n",
    "    query = \"\"\"\n",
    "    SELECT DISTINCT ?s\n",
    "        WHERE {\n",
    "          ?s <http://www.w3.org/2000/01/rdf-schema#label> ?o.\n",
    "        }\n",
    "    \"\"\"\n",
    "    entity_list = list()\n",
    "    \n",
    "    for res in graph.query(query):\n",
    "        entity_list.append(str(res[0]))\n",
    "    return entity_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4f11d8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = FILE_FOLDER + 'entity_sem_prob.csv'\n",
    "entity_sem_prob_dict = dict()\n",
    "\n",
    "if os.path.isfile(fname):\n",
    "    entity_sem_prob_df = pd.read_csv(fname)\n",
    "    for _, row in entity_sem_prob_df.iterrows():\n",
    "        e1 = row['e1']\n",
    "        e2 = row['e2']\n",
    "        sim = row['sim']\n",
    "        entity_sem_prob_dict.setdefault(e1, dict())[e2] = sim\n",
    "else:\n",
    "    graph_1_entity_list = get_graph_entities(graph_1)\n",
    "    graph_2_entity_list = get_graph_entities(graph_2)\n",
    "    \n",
    "    entity_1_df = pd.DataFrame()\n",
    "    entity_1_df['entity'] = graph_1_entity_list\n",
    "    entity_2_df = pd.DataFrame()\n",
    "    entity_2_df['entity'] = graph_2_entity_list\n",
    "    \n",
    "    entity_sem_prob_dict = calculate_prob_dict(entity_1_df, entity_2_df, 'entity')\n",
    "\n",
    "    entity_sem_prob_list = list()\n",
    "\n",
    "    for entity_1 in entity_sem_prob_dict.keys():\n",
    "        for entity_2 in entity_sem_prob_dict[entity_1].keys():\n",
    "            entity_sem_prob_list.append({\n",
    "                'e1': entity_1,\n",
    "                'e2': entity_2,\n",
    "                'sim': entity_sem_prob_dict[entity_1][entity_2]\n",
    "            })\n",
    "    entity_sem_prob_df = pd.DataFrame(entity_sem_prob_list)\n",
    "    entity_sem_prob_df.to_csv(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e4e0ae0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#threshold_graph_1_inv_func_df = threshold_graph_1_inv_func_df[threshold_graph_1_inv_func_df['predicate'].str.contains(LABEL_PREDICATE) == False]\n",
    "#threshold_graph_2_inv_func_df = threshold_graph_2_inv_func_df[threshold_graph_2_inv_func_df['predicate'].str.contains(LABEL_PREDICATE) == False]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "256a7c0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78f6429326494cbcb7db070d9b3828a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/135 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "literal_y1_dict = dict()\n",
    "used_p1_dict = dict()\n",
    "graph_1_predicate_list = list()\n",
    "\n",
    "for index, row in tqdm(threshold_graph_1_inv_func_df.iterrows(), total=len(threshold_graph_1_inv_func_df)):\n",
    "    predicate = row[PREDICATE]\n",
    "    predicate_uri = URIRef(predicate)\n",
    "\n",
    "    if sum(1 for _ in graph_1.subject_objects(predicate_uri)) < 5:\n",
    "        continue\n",
    "    \n",
    "    graph_1_predicate_list.append(predicate)\n",
    "    \n",
    "    #for s, p, o in graph_1.triples((None, predicate_uri, None)):\n",
    "    #    if type(o) != Literal or not isinstance(o, Literal) or not (o.datatype is None or o.datatype == XSD.string or o.datatype == RDF.langString):\n",
    "    #        continue\n",
    "    for s, p, o in graph_1.triples((None, predicate_uri, None)):\n",
    "\n",
    "        #if type(o) != Literal or not isinstance(o, Literal) or not (o.datatype is None or o.datatype == XSD.string or o.datatype == RDF.langString):\n",
    "        #    continue\n",
    "        \n",
    "        #if type(o) != Literal or not isinstance(o, Literal):\n",
    "        #    continue\n",
    "        if type(o) != Literal:\n",
    "            continue\n",
    "        \n",
    "        if (o.datatype is None or o.datatype == XSD.string or o.datatype == RDF.langString):\n",
    "            o_value = str(o)\n",
    "        else:\n",
    "            o_value = o.toPython()\n",
    "        #o_value = str(o)\n",
    "        if o_value not in literal_y1_dict.keys():\n",
    "            literal_y1_dict[o_value] = list()\n",
    "        y1_value_list = literal_y1_dict[o_value]\n",
    "        y1_value_list.append({\n",
    "            SUBJECT: str(s),\n",
    "            PREDICATE: str(predicate_uri)\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0fd76298",
   "metadata": {},
   "outputs": [],
   "source": [
    "def secure_triple_query(graph, s, p, o):\n",
    "    triples = list()\n",
    "    attempts = 0\n",
    "    while attempts < 10:\n",
    "        try:\n",
    "            for s, p, o in graph.triples((s, p, o)):\n",
    "                triples.append((s, p, o))\n",
    "            return triples\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            attempts += 1\n",
    "    return list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4543d3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_attributes_dict = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4b445e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "y2_matches = dict()\n",
    "graph_2_predicate_list = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39f8de5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a807201118a54e09ab4673d0a4b0ec4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56907 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "http://dbpedia.org/class/yago/Wikicat\"WeirdAl\"YankovicSongs does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatAlbumsProducedByChris\"Frenchie\"Smith does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatAlbumsProducedByRobertJohn\"Mutt\"Lange does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatAcademiciansOfTheRussianAcademyOfCinemaArtsAndSciences\"Nika\" does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatDavid\"Fathead\"NewmanAlbums does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatBilly\"Crash\"CraddockAlbums does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatBilly\"Crash\"CraddockCompilationAlbums does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatBilly\"Crash\"CraddockLiveAlbums does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatEddie\"Cleanhead\"VinsonAlbums does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatEddie\"Lockjaw\"DavisAlbums does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatEddie\"Lockjaw\"DavisLiveAlbums does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatIvan\"BoogalooJoe\"JonesAlbums does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatOran\"Juice\"JonesAlbums does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/Wikicat\"WeirdAl\"YankovicAlbums does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/Wikicat\"WeirdAl\"YankovicCompilationAlbums does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/Wikicat\"WeirdAl\"YankovicLiveAlbums does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/Wikicat\"WeirdAl\"YankovicVideoAlbums does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatAlbumsProducedByChris\"Frenchie\"Smith does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatAlbumsProducedByDonald\"Duck\"Dunn does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatAlbumsProducedByFred\"Sonic\"Smith does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatAlbumsProducedByJerome\"J-Roc\"Harmon does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatAlbumsProducedByJohn\"Charlie\"Whitney does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatAlbumsProducedByJohnny\"J\" does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatAlbumsProducedByLee\"Scratch\"Perry does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatAlbumsProducedByNoah\"40\"Shebib does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatAlbumsProducedByNoel\"Detail\"Fisher does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatAlbumsProducedByRobertJohn\"Mutt\"Lange does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatAlbumsProducedByRon\"Pigpen\"McKernan does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatAlbumsProducedByTerry\"Buzzy\"Johnson does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatAlbumsProducedByWilliam\"Mickey\"Stevenson does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatJohnny\"Guitar\"WatsonAlbums does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatJohnny\"Hammond\"SmithAlbums does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatLee\"Scratch\"PerryAlbums does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatMichael\"PrimeTime\"WilliamsAlbums does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatRoyceDa5'9\"Albums does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatSoil&\"Pimp\"SessionsAlbums does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatThe\"5\"RoyalesAlbums does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatHochschuleFürMusik\"HannsEisler\"Alumni does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatGardensByLancelot\"Capability\"Brown does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/Wikicat\"Crocodile\"DundeeFilms does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/Wikicat\"UnnamedHero\"Novels does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatDemocraticParty\"Saimnieks\"Politicians does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatEcologistParty\"TheGreens\"Politicians does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatNationalConvergence\"KwaNaKwa\"Politicians does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatAll-UkrainianUnion\"Fatherland\"Politicians does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatAll-UkrainianUnion\"Svoboda\"Politicians does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatPolishPeople'sParty\"Piast\"Politicians does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatPolishPeople'sParty\"Wyzwolenie\"Politicians does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatSocialDemocraticParty\"Harmony\"Politicians does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatUkrainianRepublicanParty\"Sobor\"Politicians does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatSongRecordingsProducedByJerome\"J-Roc\"Harmon does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatSongRecordingsProducedByNoah\"40\"Shebib does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatSongRecordingsProducedByNoel\"Detail\"Fisher does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatSongRecordingsProducedByRobertJohn\"Mutt\"Lange does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatSongRecordingsProducedBySteven\"Lenky\"Marsden does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatSongRecordingsProducedByWilliam\"Mickey\"Stevenson does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatScreenplaysBy\"WeirdAl\"Yankovic does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatMoldovan\"A\"DivisionSeasons does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatMoldovan\"B\"DivisionSeasons does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatBilly\"Crash\"CraddockSongs does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/Wikicat\"WeirdAl\"YankovicSongs does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatJohnny\"Country\"MathisSongs does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatRoyceDa5'9\"Songs does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatSongsWithLyricsBy\"WeirdAl\"Yankovic does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatThe\"5\"RoyalesSongs does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatMusicVideosDirectedBy\"WeirdAl\"Yankovic does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatDavid\"Fathead\"NewmanAlbums does not look like a valid URI, trying to serialize this will break.\n",
      "http://yago-knowledge.org/resource/wikicat_David_\"Fathead\"_Newman_albums does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatDemocraticParty\"Saimnieks\"Politicians does not look like a valid URI, trying to serialize this will break.\n",
      "http://yago-knowledge.org/resource/wikicat_Democratic_Party_\"Saimnieks\"_politicians does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatBilly\"Crash\"CraddockAlbums does not look like a valid URI, trying to serialize this will break.\n",
      "http://yago-knowledge.org/resource/wikicat_Billy_\"Crash\"_Craddock_albums does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatBilly\"Crash\"CraddockCompilationAlbums does not look like a valid URI, trying to serialize this will break.\n",
      "http://yago-knowledge.org/resource/wikicat_Billy_\"Crash\"_Craddock_compilation_albums does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatBilly\"Crash\"CraddockLiveAlbums does not look like a valid URI, trying to serialize this will break.\n",
      "http://yago-knowledge.org/resource/wikicat_Billy_\"Crash\"_Craddock_live_albums does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatBilly\"Crash\"CraddockSongs does not look like a valid URI, trying to serialize this will break.\n",
      "http://yago-knowledge.org/resource/wikicat_Billy_\"Crash\"_Craddock_songs does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatEcologistParty\"TheGreens\"Politicians does not look like a valid URI, trying to serialize this will break.\n",
      "http://yago-knowledge.org/resource/wikicat_Ecologist_Party_\"The_Greens\"_politicians does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatEddie\"Cleanhead\"VinsonAlbums does not look like a valid URI, trying to serialize this will break.\n",
      "http://yago-knowledge.org/resource/wikicat_Eddie_\"Cleanhead\"_Vinson_albums does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatEddie\"Lockjaw\"DavisAlbums does not look like a valid URI, trying to serialize this will break.\n",
      "http://yago-knowledge.org/resource/wikicat_Eddie_\"Lockjaw\"_Davis_albums does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatEddie\"Lockjaw\"DavisLiveAlbums does not look like a valid URI, trying to serialize this will break.\n",
      "http://yago-knowledge.org/resource/wikicat_Eddie_\"Lockjaw\"_Davis_live_albums does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatGardensByLancelot\"Capability\"Brown does not look like a valid URI, trying to serialize this will break.\n",
      "http://yago-knowledge.org/resource/wikicat_Gardens_by_Lancelot_\"Capability\"_Brown does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatHochschuleFürMusik\"HannsEisler\"Alumni does not look like a valid URI, trying to serialize this will break.\n",
      "http://yago-knowledge.org/resource/wikicat_Hochschule_für_Musik_\"Hanns_Eisler\"_alumni does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatIvan\"BoogalooJoe\"JonesAlbums does not look like a valid URI, trying to serialize this will break.\n",
      "http://yago-knowledge.org/resource/wikicat_Ivan_\"Boogaloo_Joe\"_Jones_albums does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatMusicVideosDirectedBy\"WeirdAl\"Yankovic does not look like a valid URI, trying to serialize this will break.\n",
      "http://yago-knowledge.org/resource/wikicat_Music_videos_directed_by_\"Weird_Al\"_Yankovic does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatNationalConvergence\"KwaNaKwa\"Politicians does not look like a valid URI, trying to serialize this will break.\n",
      "http://yago-knowledge.org/resource/wikicat_National_Convergence_\"Kwa_Na_Kwa\"_politicians does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatOran\"Juice\"JonesAlbums does not look like a valid URI, trying to serialize this will break.\n",
      "http://yago-knowledge.org/resource/wikicat_Oran_\"Juice\"_Jones_albums does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/Wikicat\"Crocodile\"DundeeFilms does not look like a valid URI, trying to serialize this will break.\n",
      "http://yago-knowledge.org/resource/wikicat_\"Crocodile\"_Dundee_films does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/Wikicat\"UnnamedHero\"Novels does not look like a valid URI, trying to serialize this will break.\n",
      "http://yago-knowledge.org/resource/wikicat_\"Unnamed_hero\"_novels does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/Wikicat\"WeirdAl\"YankovicAlbums does not look like a valid URI, trying to serialize this will break.\n",
      "http://yago-knowledge.org/resource/wikicat_\"Weird_Al\"_Yankovic_albums does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/Wikicat\"WeirdAl\"YankovicCompilationAlbums does not look like a valid URI, trying to serialize this will break.\n",
      "http://yago-knowledge.org/resource/wikicat_\"Weird_Al\"_Yankovic_compilation_albums does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/Wikicat\"WeirdAl\"YankovicLiveAlbums does not look like a valid URI, trying to serialize this will break.\n",
      "http://yago-knowledge.org/resource/wikicat_\"Weird_Al\"_Yankovic_live_albums does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/Wikicat\"WeirdAl\"YankovicSongs does not look like a valid URI, trying to serialize this will break.\n",
      "http://yago-knowledge.org/resource/wikicat_\"Weird_Al\"_Yankovic_songs does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/Wikicat\"WeirdAl\"YankovicVideoAlbums does not look like a valid URI, trying to serialize this will break.\n",
      "http://yago-knowledge.org/resource/wikicat_\"Weird_Al\"_Yankovic_video_albums does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatAcademiciansOfTheRussianAcademyOfCinemaArtsAndSciences\"Nika\" does not look like a valid URI, trying to serialize this will break.\n",
      "http://yago-knowledge.org/resource/wikicat_Academicians_of_the_Russian_Academy_of_Cinema_Arts_and_Sciences_\"Nika\" does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatAlbumsProducedByChris\"Frenchie\"Smith does not look like a valid URI, trying to serialize this will break.\n",
      "http://yago-knowledge.org/resource/wikicat_Albums_produced_by_Chris_\"Frenchie\"_Smith does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatAlbumsProducedByDonald\"Duck\"Dunn does not look like a valid URI, trying to serialize this will break.\n",
      "http://yago-knowledge.org/resource/wikicat_Albums_produced_by_Donald_\"Duck\"_Dunn does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatAlbumsProducedByFred\"Sonic\"Smith does not look like a valid URI, trying to serialize this will break.\n",
      "http://yago-knowledge.org/resource/wikicat_Albums_produced_by_Fred_\"Sonic\"_Smith does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatAlbumsProducedByJerome\"J-Roc\"Harmon does not look like a valid URI, trying to serialize this will break.\n",
      "http://yago-knowledge.org/resource/wikicat_Albums_produced_by_Jerome_\"J-Roc\"_Harmon does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatAlbumsProducedByJohn\"Charlie\"Whitney does not look like a valid URI, trying to serialize this will break.\n",
      "http://yago-knowledge.org/resource/wikicat_Albums_produced_by_John_\"Charlie\"_Whitney does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatAlbumsProducedByJohnny\"J\" does not look like a valid URI, trying to serialize this will break.\n",
      "http://yago-knowledge.org/resource/wikicat_Albums_produced_by_Johnny_\"J\" does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatAlbumsProducedByLee\"Scratch\"Perry does not look like a valid URI, trying to serialize this will break.\n",
      "http://yago-knowledge.org/resource/wikicat_Albums_produced_by_Lee_\"Scratch\"_Perry does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatAlbumsProducedByNoah\"40\"Shebib does not look like a valid URI, trying to serialize this will break.\n",
      "http://yago-knowledge.org/resource/wikicat_Albums_produced_by_Noah_\"40\"_Shebib does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatAlbumsProducedByNoel\"Detail\"Fisher does not look like a valid URI, trying to serialize this will break.\n",
      "http://yago-knowledge.org/resource/wikicat_Albums_produced_by_Noel_\"Detail\"_Fisher does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatAlbumsProducedByRobertJohn\"Mutt\"Lange does not look like a valid URI, trying to serialize this will break.\n",
      "http://yago-knowledge.org/resource/wikicat_Albums_produced_by_Robert_John_\"Mutt\"_Lange does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatAlbumsProducedByRon\"Pigpen\"McKernan does not look like a valid URI, trying to serialize this will break.\n",
      "http://yago-knowledge.org/resource/wikicat_Albums_produced_by_Ron_\"Pigpen\"_McKernan does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatAlbumsProducedByTerry\"Buzzy\"Johnson does not look like a valid URI, trying to serialize this will break.\n",
      "http://yago-knowledge.org/resource/wikicat_Albums_produced_by_Terry_\"Buzzy\"_Johnson does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatAlbumsProducedByWilliam\"Mickey\"Stevenson does not look like a valid URI, trying to serialize this will break.\n",
      "http://yago-knowledge.org/resource/wikicat_Albums_produced_by_William_\"Mickey\"_Stevenson does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatAll-UkrainianUnion\"Fatherland\"Politicians does not look like a valid URI, trying to serialize this will break.\n",
      "http://yago-knowledge.org/resource/wikicat_All-Ukrainian_Union_\"Fatherland\"_politicians does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatAll-UkrainianUnion\"Svoboda\"Politicians does not look like a valid URI, trying to serialize this will break.\n",
      "http://yago-knowledge.org/resource/wikicat_All-Ukrainian_Union_\"Svoboda\"_politicians does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatJohnny\"Country\"MathisSongs does not look like a valid URI, trying to serialize this will break.\n",
      "http://yago-knowledge.org/resource/wikicat_Johnny_\"Country\"_Mathis_songs does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatJohnny\"Guitar\"WatsonAlbums does not look like a valid URI, trying to serialize this will break.\n",
      "http://yago-knowledge.org/resource/wikicat_Johnny_\"Guitar\"_Watson_albums does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatJohnny\"Hammond\"SmithAlbums does not look like a valid URI, trying to serialize this will break.\n",
      "http://yago-knowledge.org/resource/wikicat_Johnny_\"Hammond\"_Smith_albums does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatLee\"Scratch\"PerryAlbums does not look like a valid URI, trying to serialize this will break.\n",
      "http://yago-knowledge.org/resource/wikicat_Lee_\"Scratch\"_Perry_albums does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatMichael\"PrimeTime\"WilliamsAlbums does not look like a valid URI, trying to serialize this will break.\n",
      "http://yago-knowledge.org/resource/wikicat_Michael_\"Prime_Time\"_Williams_albums does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatMoldovan\"A\"DivisionSeasons does not look like a valid URI, trying to serialize this will break.\n",
      "http://yago-knowledge.org/resource/wikicat_Moldovan_\"A\"_Division_seasons does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatMoldovan\"B\"DivisionSeasons does not look like a valid URI, trying to serialize this will break.\n",
      "http://yago-knowledge.org/resource/wikicat_Moldovan_\"B\"_Division_seasons does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatPolishPeople'sParty\"Piast\"Politicians does not look like a valid URI, trying to serialize this will break.\n",
      "http://yago-knowledge.org/resource/wikicat_Polish_People's_Party_\"Piast\"_politicians does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatPolishPeople'sParty\"Wyzwolenie\"Politicians does not look like a valid URI, trying to serialize this will break.\n",
      "http://yago-knowledge.org/resource/wikicat_Polish_People's_Party_\"Wyzwolenie\"_politicians does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatRoyceDa5'9\"Albums does not look like a valid URI, trying to serialize this will break.\n",
      "http://yago-knowledge.org/resource/wikicat_Royce_Da_5'9\"_albums does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatRoyceDa5'9\"Songs does not look like a valid URI, trying to serialize this will break.\n",
      "http://yago-knowledge.org/resource/wikicat_Royce_da_5'9\"_songs does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatScreenplaysBy\"WeirdAl\"Yankovic does not look like a valid URI, trying to serialize this will break.\n",
      "http://yago-knowledge.org/resource/wikicat_Screenplays_by_\"Weird_Al\"_Yankovic does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatSocialDemocraticParty\"Harmony\"Politicians does not look like a valid URI, trying to serialize this will break.\n",
      "http://yago-knowledge.org/resource/wikicat_Social_Democratic_Party_\"Harmony\"_politicians does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatSoil&\"Pimp\"SessionsAlbums does not look like a valid URI, trying to serialize this will break.\n",
      "http://yago-knowledge.org/resource/wikicat_Soil_&_\"Pimp\"_Sessions_albums does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatSongRecordingsProducedByJerome\"J-Roc\"Harmon does not look like a valid URI, trying to serialize this will break.\n",
      "http://yago-knowledge.org/resource/wikicat_Song_recordings_produced_by_Jerome_\"J-Roc\"_Harmon does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatSongRecordingsProducedByNoah\"40\"Shebib does not look like a valid URI, trying to serialize this will break.\n",
      "http://yago-knowledge.org/resource/wikicat_Song_recordings_produced_by_Noah_\"40\"_Shebib does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatSongRecordingsProducedByNoel\"Detail\"Fisher does not look like a valid URI, trying to serialize this will break.\n",
      "http://yago-knowledge.org/resource/wikicat_Song_recordings_produced_by_Noel_\"Detail\"_Fisher does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatSongRecordingsProducedByRobertJohn\"Mutt\"Lange does not look like a valid URI, trying to serialize this will break.\n",
      "http://yago-knowledge.org/resource/wikicat_Song_recordings_produced_by_Robert_John_\"Mutt\"_Lange does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatSongRecordingsProducedBySteven\"Lenky\"Marsden does not look like a valid URI, trying to serialize this will break.\n",
      "http://yago-knowledge.org/resource/wikicat_Song_recordings_produced_by_Steven_\"Lenky\"_Marsden does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatSongRecordingsProducedByWilliam\"Mickey\"Stevenson does not look like a valid URI, trying to serialize this will break.\n",
      "http://yago-knowledge.org/resource/wikicat_Song_recordings_produced_by_William_\"Mickey\"_Stevenson does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatSongsWithLyricsBy\"WeirdAl\"Yankovic does not look like a valid URI, trying to serialize this will break.\n",
      "http://yago-knowledge.org/resource/wikicat_Songs_with_lyrics_by_\"Weird_Al\"_Yankovic does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatThe\"5\"RoyalesAlbums does not look like a valid URI, trying to serialize this will break.\n",
      "http://yago-knowledge.org/resource/wikicat_The_\"5\"_Royales_albums does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatThe\"5\"RoyalesSongs does not look like a valid URI, trying to serialize this will break.\n",
      "http://yago-knowledge.org/resource/wikicat_The_\"5\"_Royales_songs does not look like a valid URI, trying to serialize this will break.\n",
      "http://dbpedia.org/class/yago/WikicatUkrainianRepublicanParty\"Sobor\"Politicians does not look like a valid URI, trying to serialize this will break.\n",
      "http://yago-knowledge.org/resource/wikicat_Ukrainian_Republican_Party_\"Sobor\"_politicians does not look like a valid URI, trying to serialize this will break.\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#date, Converter=<function parse_date at 0x121845700>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/rdflib/term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/isodate/isodates.py\", line 203, in parse_date\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 date format: %r' % datestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 date format: '-509-03-03'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#date, Converter=<function parse_date at 0x121845700>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/rdflib/term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/isodate/isodates.py\", line 203, in parse_date\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 date format: %r' % datestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 date format: '-323-06-11'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#date, Converter=<function parse_date at 0x121845700>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/rdflib/term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/isodate/isodates.py\", line 203, in parse_date\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 date format: %r' % datestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 date format: '-217-06-24'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#date, Converter=<function parse_date at 0x121845700>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/rdflib/term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/isodate/isodates.py\", line 203, in parse_date\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 date format: %r' % datestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 date format: '-216-08-02'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#date, Converter=<function parse_date at 0x121845700>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/rdflib/term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/isodate/isodates.py\", line 203, in parse_date\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 date format: %r' % datestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 date format: '-089-06-11'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#date, Converter=<function parse_date at 0x121845700>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/rdflib/term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/isodate/isodates.py\", line 203, in parse_date\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 date format: %r' % datestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 date format: '-082-11-01'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#date, Converter=<function parse_date at 0x121845700>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/rdflib/term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/isodate/isodates.py\", line 203, in parse_date\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 date format: %r' % datestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 date format: '-054-07-31'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#date, Converter=<function parse_date at 0x121845700>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/rdflib/term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/isodate/isodates.py\", line 203, in parse_date\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 date format: %r' % datestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 date format: '-052-01-18'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#date, Converter=<function parse_date at 0x121845700>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/rdflib/term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/isodate/isodates.py\", line 203, in parse_date\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 date format: %r' % datestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 date format: '-048-08-09'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#date, Converter=<function parse_date at 0x121845700>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/rdflib/term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/isodate/isodates.py\", line 203, in parse_date\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 date format: %r' % datestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 date format: '-045-03-17'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#date, Converter=<function parse_date at 0x121845700>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/rdflib/term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/isodate/isodates.py\", line 203, in parse_date\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 date format: %r' % datestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 date format: '-045-03-17'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#date, Converter=<function parse_date at 0x121845700>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/rdflib/term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/isodate/isodates.py\", line 203, in parse_date\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 date format: %r' % datestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 date format: '-045-04-12'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#date, Converter=<function parse_date at 0x121845700>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/rdflib/term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/isodate/isodates.py\", line 203, in parse_date\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 date format: %r' % datestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 date format: '-044-03-15'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#date, Converter=<function parse_date at 0x121845700>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/rdflib/term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/isodate/isodates.py\", line 203, in parse_date\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 date format: %r' % datestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 date format: '-044-04-13'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#date, Converter=<function parse_date at 0x121845700>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/rdflib/term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/isodate/isodates.py\", line 203, in parse_date\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 date format: %r' % datestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 date format: '-043-12-07'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#date, Converter=<function parse_date at 0x121845700>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/rdflib/term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/isodate/isodates.py\", line 203, in parse_date\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 date format: %r' % datestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 date format: '-043-12-07'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#date, Converter=<function parse_date at 0x121845700>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/rdflib/term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/isodate/isodates.py\", line 203, in parse_date\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 date format: %r' % datestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 date format: '-043-12-07'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#date, Converter=<function parse_date at 0x121845700>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/rdflib/term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/isodate/isodates.py\", line 203, in parse_date\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 date format: %r' % datestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 date format: '-043-12-07'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#date, Converter=<function parse_date at 0x121845700>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/rdflib/term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/isodate/isodates.py\", line 203, in parse_date\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 date format: %r' % datestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 date format: '-042-10-03'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#date, Converter=<function parse_date at 0x121845700>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/rdflib/term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/isodate/isodates.py\", line 203, in parse_date\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 date format: %r' % datestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 date format: '-042-10-23'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#date, Converter=<function parse_date at 0x121845700>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/rdflib/term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/isodate/isodates.py\", line 203, in parse_date\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 date format: %r' % datestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 date format: '-030-08-01'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#date, Converter=<function parse_date at 0x121845700>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/rdflib/term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/isodate/isodates.py\", line 203, in parse_date\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 date format: %r' % datestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 date format: '-030-08-23'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#date, Converter=<function parse_date at 0x121845700>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/rdflib/term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/isodate/isodates.py\", line 203, in parse_date\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 date format: %r' % datestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 date format: '-019-09-21'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#date, Converter=<function parse_date at 0x121845700>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/rdflib/term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/isodate/isodates.py\", line 203, in parse_date\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 date format: %r' % datestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 date format: '-008-11-27'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#date, Converter=<function parse_date at 0x121845700>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/rdflib/term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/isodate/isodates.py\", line 203, in parse_date\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 date format: %r' % datestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 date format: '-3102-04-25'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#date, Converter=<function parse_date at 0x121845700>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/rdflib/term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/isodate/isodates.py\", line 203, in parse_date\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 date format: %r' % datestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 date format: '-1825-05-09'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#date, Converter=<function parse_date at 0x121845700>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/rdflib/term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/isodate/isodates.py\", line 203, in parse_date\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 date format: %r' % datestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 date format: '-1796-09-28'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#date, Converter=<function parse_date at 0x121845700>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/rdflib/term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/isodate/isodates.py\", line 203, in parse_date\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 date format: %r' % datestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 date format: '-210-10-09'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#date, Converter=<function parse_date at 0x121845700>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/rdflib/term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/isodate/isodates.py\", line 203, in parse_date\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 date format: %r' % datestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 date format: '-106-01-03'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#date, Converter=<function parse_date at 0x121845700>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/rdflib/term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/isodate/isodates.py\", line 203, in parse_date\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 date format: %r' % datestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 date format: '-106-01-03'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#date, Converter=<function parse_date at 0x121845700>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/rdflib/term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/isodate/isodates.py\", line 203, in parse_date\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 date format: %r' % datestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 date format: '-106-01-03'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#date, Converter=<function parse_date at 0x121845700>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/rdflib/term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/isodate/isodates.py\", line 203, in parse_date\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 date format: %r' % datestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 date format: '-106-01-03'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#date, Converter=<function parse_date at 0x121845700>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/rdflib/term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/isodate/isodates.py\", line 203, in parse_date\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 date format: %r' % datestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 date format: '-100-07-12'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#date, Converter=<function parse_date at 0x121845700>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/rdflib/term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/isodate/isodates.py\", line 203, in parse_date\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 date format: %r' % datestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 date format: '-083-01-14'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#date, Converter=<function parse_date at 0x121845700>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/rdflib/term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/isodate/isodates.py\", line 203, in parse_date\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 date format: %r' % datestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 date format: '-081-04-27'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#date, Converter=<function parse_date at 0x121845700>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/rdflib/term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/isodate/isodates.py\", line 203, in parse_date\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 date format: %r' % datestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 date format: '-070-10-15'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#date, Converter=<function parse_date at 0x121845700>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/rdflib/term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/isodate/isodates.py\", line 203, in parse_date\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 date format: %r' % datestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 date format: '-065-12-08'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#date, Converter=<function parse_date at 0x121845700>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/rdflib/term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/isodate/isodates.py\", line 203, in parse_date\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 date format: %r' % datestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 date format: '-043-03-20'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#date, Converter=<function parse_date at 0x121845700>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/rdflib/term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "  File \"/Users/yamamotobikutorueiichi/miniconda3/envs/full-triple-matcher/lib/python3.8/site-packages/isodate/isodates.py\", line 203, in parse_date\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 date format: %r' % datestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 date format: '-002-12-11'\n"
     ]
    }
   ],
   "source": [
    "fname_matches = FILE_FOLDER + 'y2_matches.pkl'\n",
    "fname_graph_2_predicate_list = FILE_FOLDER + 'graph_2_predicate_list.pkl'\n",
    "\n",
    "if os.path.isfile(fname_matches) and os.path.isfile(fname_graph_2_predicate_list):\n",
    "    with open(fname_matches, 'rb') as f:\n",
    "        y2_matches = pickle.load(f)\n",
    "    with open(fname_graph_2_predicate_list, 'rb') as f:\n",
    "        graph_2_predicate_list = pickle.load(f)\n",
    "    print('loaded y2_matches and graph_2_predicate_list')\n",
    "else:\n",
    "    for index, row in tqdm(threshold_graph_2_inv_func_df.iterrows(), total=len(threshold_graph_2_inv_func_df)):\n",
    "        predicate = row[PREDICATE]\n",
    "        predicate_uri = URIRef(predicate)\n",
    "        \n",
    "        #if sum(1 for _ in graph_2.subject_objects(predicate_uri)) < 5:\n",
    "        #    continue\n",
    "            \n",
    "        if predicate in graph_2_predicate_list:\n",
    "            continue\n",
    "\n",
    "        graph_2_predicate_list.append(predicate)\n",
    "        \n",
    "        for s, p, o in secure_triple_query(graph_2, None, predicate_uri, None):\n",
    "            #if type(o) != Literal or not isinstance(o, Literal) or not (o.datatype is None or o.datatype == XSD.string or o.datatype == RDF.langString):\n",
    "            #    continue\n",
    "\n",
    "            subject_triples = loaded_attributes_dict.setdefault(str(s), list())\n",
    "            subject_triples.append({\n",
    "                's': str(s),\n",
    "                'p': str(p),\n",
    "                'o': o\n",
    "            })\n",
    "            \n",
    "            #if type(o) != Literal or not isinstance(o, Literal):\n",
    "            #    continue\n",
    "            if type(o) != Literal:\n",
    "                continue\n",
    "            # Check if exist match\n",
    "            #o_value = str(o)\n",
    "            if (o.datatype is None or o.datatype == XSD.string or o.datatype == RDF.langString):\n",
    "                o_value = str(o)\n",
    "            else:\n",
    "                o_value = o.toPython()\n",
    "            if o_value not in literal_y1_dict.keys():\n",
    "                continue\n",
    "                \n",
    "            if o_value not in y2_matches.keys():\n",
    "                y2_matches[o_value] = list()\n",
    "            y2_match_list = y2_matches[o_value]\n",
    "            y2_match_list.append({\n",
    "                SUBJECT: str(s),\n",
    "                PREDICATE: str(predicate_uri)\n",
    "            })\n",
    "    with open(FILE_FOLDER + 'y2_matches.pkl', 'wb') as f:  # open a text file\n",
    "        pickle.dump(y2_matches, f) # serialize the list\n",
    "    \n",
    "    with open(FILE_FOLDER + 'graph_2_predicate_list.pkl', 'wb') as f:  # open a text file\n",
    "        pickle.dump(graph_2_predicate_list, f) # serialize the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fe6f55db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40bf2c56f615407ab3309acd063f7a68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/135 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "func_1_dict = dict()\n",
    "\n",
    "for index, row in tqdm(threshold_graph_1_inv_func_df.iterrows(), total=len(threshold_graph_1_inv_func_df)):\n",
    "    func_1_dict[row['predicate']] = row[FUNCTIONALITY]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "80b5c5ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d1996edbcfa410595b6ade26c12611b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/135 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inv_func_1_dict = dict()\n",
    "\n",
    "for index, row in tqdm(threshold_graph_1_inv_func_df.iterrows(), total=len(threshold_graph_1_inv_func_df)):\n",
    "    inv_func_1_dict[row['predicate']] = row[INVERSE_FUNCTIONALITY]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "371f2b8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cc09176648b475f80118ade9bce9ca1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56907 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "func_2_dict = dict()\n",
    "\n",
    "for index, row in tqdm(threshold_graph_2_inv_func_df.iterrows(), total=len(threshold_graph_2_inv_func_df)):\n",
    "    func_2_dict[row['predicate']] = row[FUNCTIONALITY]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1dcba341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5f85ec5a3cb48d98d588fadc68988cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56907 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inv_func_2_dict = dict()\n",
    "\n",
    "for index, row in tqdm(threshold_graph_2_inv_func_df.iterrows(), total=len(threshold_graph_2_inv_func_df)):\n",
    "    inv_func_2_dict[row['predicate']] = row[INVERSE_FUNCTIONALITY]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012d52f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_sim_predicate_pair_dict = dict()\n",
    "\n",
    "for index, row in tqdm(graph_1_inv_func_df.iterrows(), total=len(graph_1_inv_func_df)):\n",
    "    p1 = row[PREDICATE]\n",
    "    \n",
    "    p1_functionality = func_1_dict[p1]\n",
    "    \n",
    "    if p1_functionality < 0.5:\n",
    "        continue\n",
    "    \n",
    "    if p1 not in predicate_prob_dict:\n",
    "        continue\n",
    "    \n",
    "    for p2 in predicate_prob_dict[p1]:\n",
    "        p2_functionality = func_2_dict[p2]\n",
    "        \n",
    "        predicate_sim = predicate_prob_dict[p1][p2]\n",
    "        \n",
    "        func_sim_value = p1_functionality * p2_functionality * predicate_sim\n",
    "        \n",
    "        if func_sim_value > 0.5:\n",
    "            high_sim_predicate_pair_dict.setdefault(p1, {})[p2] = func_sim_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fda12fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f2234b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_file():\n",
    "    global global_saved_triple_list\n",
    "    global global_iteration\n",
    "    \n",
    "    if len(global_saved_triple_list) == 0:\n",
    "        return\n",
    "    fieldnames = global_saved_triple_list[0].keys()\n",
    "    \n",
    "    # Save to CSV file\n",
    "    with open(FILE_FOLDER + \"saved_triples_\" + str(global_iteration) + \".csv\", 'a', newline='') as csv_file:\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames, delimiter='|')\n",
    "\n",
    "        # Write data\n",
    "        for row in global_saved_triple_list:\n",
    "            try:\n",
    "                writer.writerow(row)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "            \n",
    "    global_saved_triple_list = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7388b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_triples(x1, p1, y1, x2, p2, y2, sim):\n",
    "    if sim < 0.25:\n",
    "        return\n",
    "    global global_saved_triple_list\n",
    "    global_saved_triple_list.append({'x1': x1,\n",
    "                             'p1': p1,\n",
    "                             'y1': y1,\n",
    "                             'x2': x2,\n",
    "                             'p2': p2,\n",
    "                             'y2': y2,\n",
    "                             'sim': sim})\n",
    "    if len(global_saved_triple_list) > 10000:\n",
    "        save_to_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2df0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_div_triples(div_triples, x1, p1, y1, x2, p2, y2, triple_sim, save=False):\n",
    "    global global_iteration\n",
    "    if save or len(div_triples) > 10000:\n",
    "        with open(FILE_FOLDER + \"div_triples_\" + str(global_iteration) + \".csv\", 'a', newline='') as csv_file:\n",
    "            fieldnames = div_triples[0].keys()\n",
    "            writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "\n",
    "            # Write data\n",
    "            for row in global_saved_triple_list:\n",
    "                writer.writerow(row)\n",
    "        return\n",
    "    if triple_sim < 0.25:\n",
    "        return\n",
    "    div_triples.append({'x1': x1,\n",
    "                         'p1': p1,\n",
    "                         'y1': y1,\n",
    "                         'x2': x2,\n",
    "                         'p2': p2,\n",
    "                         'y2': y2,\n",
    "                         'sim': triple_sim})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8720e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_prob_entities(entity_pair_match_dict, x1, x2, p1, p2, prob_x, prob_y):\n",
    "    y1_pred = str(p1)\n",
    "    y2_pred = str(p2)\n",
    "    if y1_pred not in predicate_prob_dict or y2_pred not in predicate_prob_dict[y1_pred]:\n",
    "        return 0.0, 0.0\n",
    "    pred_sim = predicate_prob_dict[y1_pred][y2_pred]\n",
    "    \n",
    "    prob_func = prob_x * pred_sim * func_1_dict[y1_pred] * func_2_dict[y2_pred] * prob_y\n",
    "    prob_inv_func = prob_x * pred_sim * inv_func_1_dict[y1_pred] * inv_func_2_dict[y2_pred] * prob_y\n",
    "    new_factor = (1-prob_func) * (1-prob_inv_func)\n",
    "    \n",
    "    if new_factor > 0.99:\n",
    "        return 0.0, 0.0\n",
    "    \n",
    "    if x1 not in entity_pair_match_dict.keys():\n",
    "        entity_pair_match_dict[x1] = dict()\n",
    "    if x2 not in entity_pair_match_dict[x1].keys():\n",
    "        entity_pair_match_dict[x1][x2] = 1\n",
    "\n",
    "    entity_pair_match_dict[x1][x2] *= (1-prob_func) * (1-prob_inv_func)\n",
    "    \n",
    "    prob_triple = prob_func\n",
    "    if prob_inv_func > prob_func:\n",
    "        prob_triple = prob_inv_func\n",
    "\n",
    "    return 1 - entity_pair_match_dict[x1][x2], 1 - new_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41df8782",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entity_prob_using_attribute(entity_triple_match, entity_pair_dict):\n",
    "    for o_value, y2_match_list in tqdm(y2_matches.items(), desc='Attribute loop', leave=False):\n",
    "        y1_match_list = literal_y1_dict[o_value]\n",
    "\n",
    "        min_size_for_tqdm = 100  # Define your threshold here\n",
    "\n",
    "        if len(y1_match_list) > min_size_for_tqdm:\n",
    "            iterable = tqdm(y1_match_list, desc='y1_match_list', leave=False)\n",
    "        else:\n",
    "            iterable = y1_match_list\n",
    "        for y1_match in iterable:\n",
    "            y1_subject = str(y1_match['subject'])\n",
    "\n",
    "            for y2_match in y2_match_list:\n",
    "                y2_subject = str(y2_match['subject'])\n",
    "\n",
    "                #if y1_subject in entity_pair_dict and y2_subject in entity_pair_dict[y1_subject]:\n",
    "                y1_pred = str(y1_match['predicate'])\n",
    "                y2_pred = str(y2_match['predicate'])\n",
    "                \n",
    "                prob_x = 0.35\n",
    "                if y1_subject in entity_pair_dict and y2_subject in entity_pair_dict[y1_subject]:\n",
    "                    prob_x = entity_pair_dict[y1_subject][y2_subject]\n",
    "                \n",
    "                # Here we only update subject, because objects are literals\n",
    "                sim, triple_sim = update_prob_entities(entity_triple_match, str(y1_subject), str(y2_subject),\n",
    "                                     y1_pred, y2_pred, prob_x, 1)\n",
    "                \n",
    "                save_triples(y1_subject, y1_pred, o_value, y2_subject, y2_pred, o_value, triple_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dcb6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbors(graph, predicate_list, y, loaded_neighbors_dict, inbound=True):\n",
    "    if not y:\n",
    "        return []\n",
    "    \n",
    "    if y in loaded_neighbors_dict.keys():\n",
    "        return loaded_neighbors_dict[y]\n",
    "    \n",
    "    y_url = y\n",
    "    if type(y_url) != URIRef:\n",
    "        y_url = URIRef(y)\n",
    "    \n",
    "    attempts = 0\n",
    "    while attempts < 10:\n",
    "        try:\n",
    "            neighbor_list = list()\n",
    "            if inbound:\n",
    "                for s, p in graph.subject_predicates(y_url):\n",
    "                    if str(p) in predicate_list:\n",
    "                        neighbor_list.append({\n",
    "                            'p': str(p),\n",
    "                            's': str(s)\n",
    "                        })\n",
    "            else:\n",
    "                for p, o in graph.predicate_objects(y_url):\n",
    "                    if str(p) in predicate_list:\n",
    "                        neighbor_list.append({\n",
    "                            'p': str(p),\n",
    "                            'o': o\n",
    "                        })\n",
    "            return neighbor_list\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            attempts += 1\n",
    "    loaded_neighbors_dict[y] = neighbor_list\n",
    "    return neighbor_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb75741",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entity_pair_sim_with_default(entity_pair_dict, e1, e2):\n",
    "    if e1 in entity_pair_dict and e2 in entity_pair_dict[e1]:\n",
    "        return entity_pair_dict[e1][e2]\n",
    "    return 0.35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8d59d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_triples_with_both_side_as_entities(entity_triple_match, entity_pair_dict, s1, s2, p1, p2, o1, o2):\n",
    "    s_sim = get_entity_pair_sim_with_default(entity_pair_dict, s1, s2)\n",
    "    o_sim = get_entity_pair_sim_with_default(entity_pair_dict, o1, o2)\n",
    "    sim, triple_sim = update_prob_entities(entity_triple_match, s1, s2, p1, p2, s_sim, o_sim)\n",
    "    sim, triple_sim = update_prob_entities(entity_triple_match, o1, o2, p1, p2, s_sim, o_sim)\n",
    "    save_triples(s1, p1, o1, s2, p2, o2, triple_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435f1307",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_prob_entity_neighbors(entity_triple_match, entity_pair_dict, y1_neighbors,\n",
    "                                 y2_neighbors, y_sim, y1, y2):\n",
    "    for x1 in y1_neighbors:\n",
    "        for x2 in y2_neighbors:\n",
    "            x1_str = str(x1['s'])\n",
    "            x2_str = str(x2['s'])\n",
    "            p1_str = str(x1['p'])\n",
    "            p2_str = str(x2['p'])\n",
    "            x_sim = 0.35\n",
    "            if x1_str in entity_pair_dict and x2_str in entity_pair_dict[x1_str]:\n",
    "                x_sim = entity_pair_dict[x1_str][x2_str]\n",
    "            \n",
    "            save_triples_with_both_side_as_entities(entity_triple_match, entity_pair_dict, x1_str, x2_str,\n",
    "                                                   p1_str, p2_str, y1, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5698cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entity_prob_using_neighbors(entity_triple_match, entity_pair_dict, loaded_neighbors_dict):\n",
    "    y1_keys = list(entity_pair_dict.keys())\n",
    "    predicate_1_list = list(graph_1_inv_func_df[graph_1_inv_func_df[INVERSE_FUNCTIONALITY] > 0.25][PREDICATE])\n",
    "    predicate_2_list = list(graph_2_inv_func_df[graph_2_inv_func_df[INVERSE_FUNCTIONALITY] > 0.25][PREDICATE])\n",
    "\n",
    "    for y1 in tqdm(y1_keys, desc='neighbor loop', leave=False):\n",
    "        y1_neighbors = get_neighbors(graph_1, predicate_1_list, y1, loaded_neighbors_dict, True)\n",
    "        \n",
    "        entity_2_top_n_list = get_top_n(entity_pair_dict[y1], 10)\n",
    "\n",
    "        for y2, y_sim in entity_2_top_n_list:\n",
    "            #if y_sim < 0.25:\n",
    "            #    continue\n",
    "            \n",
    "            y2_neighbors = get_neighbors(graph_2, predicate_2_list, y2, loaded_neighbors_dict, True)\n",
    "            update_prob_entity_neighbors(entity_triple_match, entity_pair_dict,\n",
    "                                         y1_neighbors, y2_neighbors, y_sim, y1, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2782889",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_attributes(entity_triple_match, entity_pair_dict, e1_attributes,\n",
    "                    e2_attributes, e_sim, e1, e2, div_triples):\n",
    "    for e1_attribute in e1_attributes:\n",
    "        p1_str = str(e1_attribute['p'])\n",
    "        o1 = e1_attribute['o']\n",
    "        for e2_attribute in e2_attributes:\n",
    "            p2_str = str(e2_attribute['p'])\n",
    "            o2 = e2_attribute['o']\n",
    "            o_sim = 0.0\n",
    "            if type(o1) == URIRef and type(o2) == URIRef:\n",
    "                o1_str = str(o1)\n",
    "                o2_str = str(o2)\n",
    "                if o1_str in entity_pair_dict and o2_str in entity_pair_dict[o1_str]:\n",
    "                    o_sim = entity_pair_dict[o1_str][o2_str]\n",
    "                else:\n",
    "                    o_sim = 0.35\n",
    "                save_triples_with_both_side_as_entities(entity_triple_match, entity_pair_dict,\n",
    "                                                       e1, e2, p1_str, p2_str, o1_str, o2_str)\n",
    "            else:\n",
    "                o_sim = graph_similarity.get_objects_similarity(o1, o2)\n",
    "\n",
    "                sim, triple_sim = update_prob_entities(entity_triple_match, e1, e2, p1_str, p2_str, e_sim, o_sim)\n",
    "                save_triples(e1, p1_str, o1, e2, p2_str, o2, triple_sim)\n",
    "\n",
    "            #div_sim = 0.0\n",
    "            #if o_sim != 0:\n",
    "            #    div_sim = triple_sim / o_sim * o_div\n",
    "            #save_div_triples(div_triples, e1, p1_str, o1, e2, p2_str, o2, div_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce6e176",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entity_prob_using_all_attributes(entity_triple_match, entity_pair_dict, loaded_attributes_dict):\n",
    "    e1_keys = list(entity_pair_dict.keys())\n",
    "    div_triples = list()\n",
    "    predicate_1_list = list(graph_1_inv_func_df[ (graph_1_inv_func_df[FUNCTIONALITY] > 0.8) |\n",
    "                                                (graph_1_inv_func_df[INVERSE_FUNCTIONALITY] > 0.8)][PREDICATE])\n",
    "    predicate_2_list = list(graph_2_inv_func_df[ (graph_2_inv_func_df[FUNCTIONALITY] > 0.8) |\n",
    "                                                (graph_2_inv_func_df[INVERSE_FUNCTIONALITY] > 0.8)][PREDICATE])\n",
    "    for e1 in tqdm(e1_keys, desc='All attributes loop', leave=False):\n",
    "        e1_attributes = get_neighbors(graph_1, predicate_1_list, e1, loaded_attributes_dict, False)\n",
    "        \n",
    "        entity_2_top_n_list = get_top_n(entity_pair_dict[e1], 10)\n",
    "        \n",
    "        for e2, e_sim in entity_2_top_n_list:\n",
    "            \n",
    "            e2_attributes = get_neighbors(graph_2, predicate_2_list, e2, loaded_attributes_dict, False)\n",
    "            match_attributes(entity_triple_match, entity_pair_dict, e1_attributes, e2_attributes, e_sim,\n",
    "                             e1, e2, div_triples)\n",
    "            \n",
    "    #save_div_triples(div_triples, None, None, None, None, None, None, None, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763ed52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob_entity_pair(entity_pair_match_dict, s1, s2):\n",
    "    if s1 in entity_pair_match_dict.keys() and s2 in entity_pair_match_dict[s1].keys():\n",
    "        return 1 - entity_pair_match_dict[s1][s2]\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db60275",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_columns(reversed_bool):\n",
    "    if reversed_bool:\n",
    "        return {\n",
    "            'x1': 'x2',\n",
    "            'p1': 'p2',\n",
    "            'y1': 'y2',\n",
    "            'x2': 'x1',\n",
    "            'p2': 'p1',\n",
    "            'y2': 'y1'\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            'x1': 'x1',\n",
    "            'p1': 'p1',\n",
    "            'y1': 'y1',\n",
    "            'x2': 'x2',\n",
    "            'p2': 'p2',\n",
    "            'y2': 'y2'\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b2bb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_product(matched_row, entity_pair_match_dict):\n",
    "    x_sim = 1 - entity_pair_match_dict[matched_row['x1']][matched_row['x2']]\n",
    "    if matched_row['y1'] == matched_row['y2']:\n",
    "        y_sim = 1\n",
    "    else:\n",
    "        y_sim = entity_pair_match_dict[matched_row['y1']][matched_row['y2']]\n",
    "    return 1 - x_sim * y_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1a17b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sum_by_triple(indexed_df, triple_row):\n",
    "    matched_triples = indexed_df.join(triple_row)\n",
    "    return 1.0 - numpy.product(matched_triples['product_element'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20148c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_x_sim(x1, x2, entity_pair_match_dict, reversed_bool):\n",
    "    if reversed_bool:\n",
    "        return 1 - entity_pair_match_dict[x2][x1]\n",
    "    return 1 - entity_pair_match_dict[x1][x2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ac1886",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_y_sim(y1, y2, entity_pair_match_dict, reversed_bool):\n",
    "    if y1 == y2:\n",
    "        return 1\n",
    "    else:\n",
    "        if reversed_bool:\n",
    "            if y2 in entity_pair_match_dict.keys() and y1 in entity_pair_match_dict[y2].keys():\n",
    "                return 1 - entity_pair_match_dict[y2][y1]\n",
    "        if y1 in entity_pair_match_dict.keys() and y2 in entity_pair_match_dict[y1].keys():\n",
    "            return 1 - entity_pair_match_dict[y1][y2]\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26526ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subject_object(graph, predicate):\n",
    "    attempts = 0\n",
    "    while attempts < 10:\n",
    "        try:\n",
    "            s_o_list = list()\n",
    "            for s, o in graph.subject_objects(URIRef(predicate)):\n",
    "                s_o_list.append({\n",
    "                        'subject': s,\n",
    "                        'object': o\n",
    "                    })\n",
    "            return s_o_list\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            attempts += 1\n",
    "    \n",
    "    return s_o_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6f5fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_string(y):\n",
    "    return type(y) == Literal and y.value and type(y.value) == str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d2e7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sub_pred_product(entity_pair_match_dict, x1, x2, y1, y2):\n",
    "    if check_if_string(y1) and check_if_string(y2):\n",
    "        if y1.value != y2.value:\n",
    "            return 0.0\n",
    "        y_value = y1.value\n",
    "        if y_value not in literal_y1_dict.keys() or y_value not in y2_matches.keys():\n",
    "            return 0.0\n",
    "        return entity_pair_match_dict[str(x1)][str(x2)]\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57eccbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_entity_pairs(i, entity_pair_dict):\n",
    "    entity_pair_sim_list = list()\n",
    "    for y1 in entity_pair_dict.keys():\n",
    "        for y2 in entity_pair_dict[y1].keys():\n",
    "            if entity_pair_dict[y1][y2] < 0.01:\n",
    "                continue\n",
    "            entity_pair_sim_list.append({\n",
    "                \"e1\": y1,\n",
    "                \"e2\": y2,\n",
    "                \"sim\": entity_pair_dict[y1][y2]\n",
    "            })\n",
    "    entity_sim_df = pd.DataFrame(entity_pair_sim_list)\n",
    "    entity_sim_df.to_csv(FILE_FOLDER + \"entity_sim_\" + str(i) + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e09483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entity_vec(graph, entity):\n",
    "    if entity in label_dict:\n",
    "        return label_dict[entity]\n",
    "    predicate = URIRef(LABEL_PREDICATE)\n",
    "    entity_uri = URIRef(entity)\n",
    "    \n",
    "    label = str(next(graph.objects(entity_uri, predicate), \"\"))\n",
    "    \n",
    "    if not label:\n",
    "        return None\n",
    "    \n",
    "    vec = embed_long_sentence(label)\n",
    "    label_dict[entity] = vec\n",
    "    \n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856dfc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entity_label_bert_similarity(graph_1, graph_2, entity_1, entity_2):\n",
    "    vec_1 = get_entity_vec(graph_1, entity_1)\n",
    "    vec_2 = get_entity_vec(graph_2, entity_2)\n",
    "    \n",
    "    if vec_1 is None or vec_2 is None:\n",
    "        return 0.0\n",
    "    \n",
    "    similarities = cosine_similarity(vec_1, vec_2)\n",
    "    return similarities.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0883885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_calc_entity_pair_dict(entity_sem_prob_dict, entity_triple_match):\n",
    "    entity_prob_dict = dict()\n",
    "    \n",
    "    for entity_1, entity_2_dict in entity_sem_prob_dict.items():\n",
    "        for entity_2 in entity_2_dict:\n",
    "            entity_prob_dict.setdefault(entity_1, {})[entity_2] = 0.5 * entity_sem_prob_dict[entity_1][entity_2]\n",
    "    \n",
    "    for entity_1, entity_2_dict in tqdm(entity_triple_match.items(), desc='entity_pair', leave=False):\n",
    "        entity_2_top_n_list = get_top_n(entity_2_dict, 10, False)\n",
    "        #entity_2_top_n_list = get_top_n(entity_2_dict, 5, False)\n",
    "        \n",
    "        for entity_2, triple_match_value in entity_2_top_n_list:\n",
    "            entity_triple_sim = 1 - triple_match_value\n",
    "            \n",
    "            if entity_2 not in entity_prob_dict.setdefault(entity_1, {}):\n",
    "                #if entity_triple_sim < 0.2:\n",
    "                #    entity_prob_dict.setdefault(entity_1, {})[entity_2] = entity_triple_sim\n",
    "                #else:\n",
    "                bert_sim = 0.8 * entity_label_bert_similarity(graph_1, graph_2, entity_1, entity_2)\n",
    "                entity_sem_prob_dict.setdefault(entity_1, {})[entity_2] = bert_sim\n",
    "                entity_prob_dict.setdefault(entity_1, {})[entity_2] = 0.5 * bert_sim\n",
    "            entity_prob_dict.setdefault(entity_1, {})[entity_2] += 0.5 * entity_triple_sim\n",
    "            \n",
    "    return entity_prob_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b105765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n(dictionary, n, reverse=True):\n",
    "    sorted_elements = sorted(dictionary.items(), key=lambda x: x[1], reverse=reverse)\n",
    "    if len(sorted_elements) < n:\n",
    "        return sorted_elements\n",
    "    n_sim = sorted_elements[n-1][1]\n",
    "    \n",
    "    top_n_list = list()\n",
    "    for element in sorted_elements:\n",
    "        e2, sim = element\n",
    "        if (reverse and sim < n_sim) or (not reverse and sim > n_sim):\n",
    "            return top_n_list[:20]\n",
    "            \n",
    "        top_n_list.append(element)\n",
    "    return top_n_list[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab017215-3357-40e1-b7c5-92c7e6e9f9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_triple_pairs(i, entity_pair_dict):\n",
    "    entity_pair_sim_list = list()\n",
    "    for y1 in entity_pair_dict.keys():\n",
    "        for y2 in entity_pair_dict[y1].keys():\n",
    "            if entity_pair_dict[y1][y2] < 0.01:\n",
    "                continue\n",
    "            entity_pair_sim_list.append({\n",
    "                \"e1\": y1,\n",
    "                \"e2\": y2,\n",
    "                \"sim\": entity_pair_dict[y1][y2]\n",
    "            })\n",
    "    entity_sim_df = pd.DataFrame(entity_pair_sim_list)\n",
    "    entity_sim_df.to_csv(FILE_FOLDER + \"triple_sim_\" + str(i) + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962e29fa-c826-45c5-9567-ac7cf1a3aa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_df_to_entity_dict(df):\n",
    "    grouped_df = df.groupby('e1')\n",
    "    elem_dict = dict()\n",
    "    for e1, group_indices in tqdm(grouped_df.groups.items()):\n",
    "        # Access the group corresponding to 'e1'\n",
    "        e1_group = df.loc[group_indices]\n",
    "        e1_dict = dict(zip(e1_group['e2'], e1_group['sim']))\n",
    "        elem_dict[e1] = e1_dict\n",
    "    return elem_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fcdc28-0a4f-417d-a81a-a64a12d23ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_neighbors_dict = dict()\n",
    "pred_sub_relation_1 = dict()\n",
    "pred_sub_relation_2 = dict()\n",
    "entity_triple_match = dict()\n",
    "label_dict = dict()\n",
    "entity_pair_dict = pre_calc_entity_pair_dict(entity_sem_prob_dict, entity_triple_match)\n",
    "global_iteration = 0\n",
    "global_saved_triple_list = list()\n",
    "loaded_attributes_dict = dict()\n",
    "\n",
    "for i in tqdm(range(10), desc='Main loop'):\n",
    "    global_iteration = i\n",
    "    \n",
    "    calculate_entity_prob_using_attribute(entity_triple_match, entity_pair_dict)     \n",
    "    \n",
    "    calculate_entity_prob_using_neighbors(entity_triple_match, entity_pair_dict, loaded_neighbors_dict)\n",
    "    calculate_entity_prob_using_all_attributes(entity_triple_match, entity_pair_dict, loaded_attributes_dict)\n",
    "\n",
    "    save_triple_pairs(i, entity_triple_match)\n",
    "    entity_pair_dict = pre_calc_entity_pair_dict(entity_sem_prob_dict, entity_triple_match)\n",
    "    entity_triple_match = dict()\n",
    "    save_to_file()\n",
    "\n",
    "    if i > 0:\n",
    "        previous_entity_df = pd.read_csv(FILE_FOLDER + \"entity_sim_\" + str(i-1) + \".csv\")\n",
    "        previous_entity_dict = convert_df_to_entity_dict(previous_entity_df)\n",
    "        if len(entity_pair_dict.keys()) > 1.1 * len(previous_entity_dict.keys()):\n",
    "            save_entity_pairs(i, entity_pair_dict)\n",
    "            continue\n",
    "        changed_top_1_counter = 0\n",
    "        for e1, e1_dict in entity_pair_dict.items():\n",
    "            if e1 not in entity_pair_dict.keys():\n",
    "                changed_top_1_counter += 1\n",
    "                continue\n",
    "            previous_top_1 = get_top_n(e1_dict, 1)[0]\n",
    "            current_top_1 = get_top_n(entity_pair_dict[e1], 1)[0]\n",
    "            if previous_top_1 != current_top_1:\n",
    "                changed_top_1_counter += 1\n",
    "        if changed_top_1_counter < (0.1 * len(previous_entity_dict.keys())):\n",
    "            break\n",
    "    save_entity_pairs(i, entity_pair_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b573fe16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
